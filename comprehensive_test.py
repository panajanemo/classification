#!/usr/bin/env python3
"""
ç»¼åˆåŠŸèƒ½æµ‹è¯•è„šæœ¬ - å±•ç¤ºå®Œæ•´çš„è¯­ä¹‰åˆ†æ®µç³»ç»Ÿ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.enhanced_semantic_segmenter import EnhancedSemanticSegmenter
from src.core.transformer_segmenter import TransformerSemanticSegmenter
from src.core.enhanced_multi_task_segmenter import EnhancedMultiTaskSegmenter
from src.core.hybrid_segmenter import HybridSemanticSegmenter
from src.core.topic_consistency_evaluator import TopicConsistencyEvaluator
from src.core.sentence_transformer_model import SentenceTransformerModel
from src.core.text_type_detector import TextTypeDetector
from src.utils.text_processor import TextProcessor
import time
import torch


def print_separator(title: str):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("\n" + "="*80)
    print(f"ğŸ¯ {title}")
    print("="*80)


def print_section(title: str):
    """æ‰“å°å°èŠ‚æ ‡é¢˜"""
    print(f"\nğŸ“‹ {title}")
    print("-"*60)


def test_all_segmentation_methods():
    """æµ‹è¯•æ‰€æœ‰åˆ†æ®µæ–¹æ³•"""
    print_separator("ç»¼åˆè¯­ä¹‰åˆ†æ®µç³»ç»Ÿæµ‹è¯•")
    
    # æµ‹è¯•æ–‡æœ¬ - åŒ…å«å¤šç§ä¸»é¢˜çš„å¤æ‚æ–‡æœ¬
    test_text = """
    è§ç‚ç¼“ç¼“çå¼€åŒçœ¼ï¼Œçœ¼å‰ä¾ç„¶æ˜¯ç†Ÿæ‚‰çš„çŸ³å®¤ã€‚æ·¡æ·¡çš„è¯é¦™åœ¨ç©ºæ°”ä¸­å¼¥æ¼«ï¼Œè®©äººç²¾ç¥ä¸ºä¹‹ä¸€æŒ¯ã€‚ä»–ä»çŸ³åºŠä¸Šåèµ·èº«æ¥ï¼Œæ´»åŠ¨äº†ä¸€ä¸‹æœ‰äº›åƒµç¡¬çš„ç­‹éª¨ã€‚
    
    "ä½ ç»ˆäºé†’äº†ã€‚"è¯è€çš„å£°éŸ³åœ¨è€³è¾¹å“èµ·ï¼Œå¸¦ç€ä¸€ä¸æ·¡æ·¡çš„ç¬‘æ„ã€‚"è¿™æ¬¡çš„ä¿®ç‚¼æ•ˆæœå¦‚ä½•ï¼Ÿ"
    
    è§ç‚æ„Ÿå—ç€ä½“å†…æ¶ŒåŠ¨çš„æ–—æ°”ï¼Œçœ¼ä¸­é—ªè¿‡ä¸€æŠ¹å–œè‰²ã€‚ç»è¿‡è¿™æ®µæ—¶é—´çš„è‹¦ä¿®ï¼Œä»–çš„å®åŠ›ç¡®å®æœ‰äº†é•¿è¶³çš„è¿›æ­¥ã€‚
    
    ç„¶è€Œï¼Œè®©æˆ‘ä»¬æš‚æ—¶ç¦»å¼€è¿™ä¸ªå¥‡å¹»çš„ä¸–ç•Œï¼Œæ¥çœ‹çœ‹ç°å®ä¸­çš„ç§‘æŠ€å‘å±•ã€‚æœ€è¿‘äººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ï¼ŒChatGPTã€Claudeç­‰å¤§è¯­è¨€æ¨¡å‹çš„å‡ºç°ï¼Œæ ‡å¿—ç€AIæŠ€æœ¯è¿›å…¥äº†æ–°çš„æ—¶ä»£ã€‚
    
    è¿™äº›AIç³»ç»Ÿèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€ï¼Œè¿›è¡Œå¤æ‚çš„å¯¹è¯ï¼Œç”šè‡³ååŠ©ç¼–ç¨‹ã€å†™ä½œã€æ•°æ®åˆ†æç­‰å·¥ä½œã€‚å®ƒä»¬çš„å‡ºç°æ­£åœ¨æ·±åˆ»æ”¹å˜ç€æˆ‘ä»¬çš„å·¥ä½œå’Œç”Ÿæ´»æ–¹å¼ã€‚
    
    åœ¨æŠ€æœ¯å®ç°å±‚é¢ï¼Œè¿™äº›å¤§æ¨¡å‹åŸºäºTransformeræ¶æ„ï¼Œä½¿ç”¨äº†æ•°ä¸‡äº¿å‚æ•°ï¼Œç»è¿‡æµ·é‡æ–‡æœ¬æ•°æ®çš„é¢„è®­ç»ƒã€‚Pythonä½œä¸ºä¸»è¦çš„å¼€å‘è¯­è¨€ï¼ŒPyTorchå’ŒTensorFlowç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸ºæ¨¡å‹å¼€å‘æä¾›äº†å¼ºå¤§æ”¯æŒã€‚
    
    å½“ç„¶ï¼ŒAIæŠ€æœ¯çš„å‘å±•ä¹Ÿå¸¦æ¥äº†æ–°çš„æ€è€ƒã€‚å¦‚ä½•ç¡®ä¿AIçš„å®‰å…¨æ€§ã€å¯è§£é‡Šæ€§ï¼Œå¦‚ä½•å¤„ç†AIå¸¦æ¥çš„ä¼¦ç†é—®é¢˜ï¼Œè¿™äº›éƒ½æ˜¯æˆ‘ä»¬éœ€è¦è®¤çœŸé¢å¯¹çš„æŒ‘æˆ˜ã€‚
    
    å›åˆ°è§ç‚çš„ä¸–ç•Œï¼Œä»–æ­£åœ¨æ€è€ƒå¦‚ä½•å°†ç°ä»£ç§‘æŠ€çš„ç†å¿µèå…¥åˆ°æ–—æ°”ä¿®ç‚¼ä¸­ã€‚æˆ–è®¸ï¼Œæ•°æ®åˆ†æçš„æ–¹æ³•å¯ä»¥å¸®åŠ©ä»–æ›´å¥½åœ°ç†è§£åŠŸæ³•çš„å¥¥ç§˜ï¼Œç®—æ³•ä¼˜åŒ–çš„æ€è·¯å¯ä»¥æŒ‡å¯¼ä»–æ”¹è¿›ä¿®ç‚¼æ•ˆç‡ã€‚
    
    "æœ‰è¶£çš„æƒ³æ³•ã€‚"è¯è€ä¼¼ä¹è¯»æ‡‚äº†è§ç‚çš„å¿ƒæ€ï¼Œ"å¤ä»Šç»“åˆï¼Œæˆ–è®¸èƒ½äº§ç”Ÿæ„æƒ³ä¸åˆ°çš„æ•ˆæœã€‚"
    """
    
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"ğŸ“Š é¢„ä¼°ä¸»é¢˜: å¥‡å¹»å°è¯´ + äººå·¥æ™ºèƒ½æŠ€æœ¯ + å¤ä»Šç»“åˆ")
    
    # åˆå§‹åŒ–æ‰€æœ‰åˆ†æ®µå™¨
    print_section("åˆå§‹åŒ–åˆ†æ®µå™¨")
    
    model = SentenceTransformerModel(device='mps')
    text_processor = TextProcessor()
    type_detector = TextTypeDetector()
    
    # å„ç§åˆ†æ®µå™¨
    enhanced_segmenter = EnhancedSemanticSegmenter(model, text_processor, type_detector)
    transformer_segmenter = TransformerSemanticSegmenter(model, text_processor, type_detector, 'mps')
    multi_task_segmenter = EnhancedMultiTaskSegmenter(model, text_processor, type_detector, 'cpu')
    hybrid_segmenter = HybridSemanticSegmenter(model, text_processor, type_detector, 'mps')
    consistency_evaluator = TopicConsistencyEvaluator(model, text_processor, "chinese")
    
    print("âœ… æ‰€æœ‰åˆ†æ®µå™¨åˆå§‹åŒ–å®Œæˆ")
    
    # æµ‹è¯•å„ç§æ–¹æ³•
    results = {}
    
    print_section("æ–¹æ³•1: å¢å¼ºç‰ˆå¤šå°ºåº¦åˆ†æ®µå™¨")
    start_time = time.time()
    enhanced_result = enhanced_segmenter.segment_text_enhanced(test_text)
    enhanced_time = time.time() - start_time
    
    if "error" not in enhanced_result:
        results["enhanced"] = enhanced_result
        print(f"âœ… åˆ†æ®µæˆåŠŸ: {len(enhanced_result['paragraphs'])}æ®µ")
        print(f"ğŸ“Š æ–‡æœ¬ç±»å‹: {enhanced_result['text_type']} (ç½®ä¿¡åº¦: {enhanced_result['type_confidence']:.3f})")
        print(f"â­ è´¨é‡åˆ†æ•°: {enhanced_result['quality']['quality_score']:.3f}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {enhanced_time:.3f}s")
    else:
        print(f"âŒ å¤±è´¥: {enhanced_result['error']}")
    
    print_section("æ–¹æ³•2: TransformerÂ²æ¶æ„åˆ†æ®µå™¨")
    start_time = time.time()
    transformer_result = transformer_segmenter.segment_text_transformer(test_text)
    transformer_time = time.time() - start_time
    
    if "error" not in transformer_result:
        results["transformer"] = transformer_result
        print(f"âœ… åˆ†æ®µæˆåŠŸ: {len(transformer_result['paragraphs'])}æ®µ")
        print(f"ğŸ“Š æ–‡æœ¬ç±»å‹: {transformer_result['text_type']} (ç½®ä¿¡åº¦: {transformer_result['type_confidence']:.3f})")
        print(f"â­ è´¨é‡åˆ†æ•°: {transformer_result['quality']['quality_score']:.3f}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {transformer_time:.3f}s")
    else:
        print(f"âŒ å¤±è´¥: {transformer_result['error']}")
    
    print_section("æ–¹æ³•3: å¤šä»»åŠ¡å­¦ä¹ åˆ†æ®µå™¨")
    start_time = time.time()
    multi_task_result = multi_task_segmenter.segment_text_multi_task(test_text)
    multi_task_time = time.time() - start_time
    
    if "error" not in multi_task_result:
        results["multi_task"] = multi_task_result
        print(f"âœ… åˆ†æ®µæˆåŠŸ: {len(multi_task_result['paragraphs'])}æ®µ")
        print(f"ğŸ“Š æ–‡æœ¬ç±»å‹: {multi_task_result['text_type']} (ç½®ä¿¡åº¦: {multi_task_result['type_confidence']:.3f})")
        print(f"â­ è´¨é‡åˆ†æ•°: {multi_task_result['quality']['quality_score']:.3f}")
        print(f"ğŸ§  æ¨¡å‹çŠ¶æ€: {'å·²è®­ç»ƒ' if multi_task_result['is_trained'] else 'æœªè®­ç»ƒ'}")
        print(f"â±ï¸  å¤„ç†æ—¶é—´: {multi_task_time:.3f}s")
    else:
        print(f"âŒ å¤±è´¥: {multi_task_result['error']}")
    
    print_section("æ–¹æ³•4: æ··åˆæ™ºèƒ½åˆ†æ®µå™¨")
    
    # æµ‹è¯•ä¸åŒç­–ç•¥
    methods = ["enhanced", "transformer", "hybrid", "auto"]
    hybrid_results = {}
    
    for method in methods:
        start_time = time.time()
        hybrid_result = hybrid_segmenter.segment_text(test_text, method)
        method_time = time.time() - start_time
        
        if "error" not in hybrid_result:
            hybrid_results[method] = hybrid_result
            print(f"âœ… {method}: {len(hybrid_result['paragraphs'])}æ®µ, è´¨é‡{hybrid_result['quality']['quality_score']:.3f}, {method_time:.3f}s")
        else:
            print(f"âŒ {method}: {hybrid_result['error']}")
    
    # æ··åˆåˆ†æ®µå™¨æ–¹æ³•æ¯”è¾ƒ
    print("\nğŸ”„ æ··åˆåˆ†æ®µå™¨è‡ªåŠ¨æ¯”è¾ƒ:")
    comparison = hybrid_segmenter.compare_methods(test_text)
    if "error" not in comparison:
        comp_summary = comparison["comparison"]
        print(f"ğŸ† æœ€ä½³æ–¹æ³•: {comp_summary['best_method']}")
        if 'quality_comparison' in comp_summary:
            print("ğŸ“ˆ è´¨é‡æ’å:")
            sorted_methods = sorted(comp_summary['quality_comparison'].items(), 
                                  key=lambda x: x[1], reverse=True)
            for rank, (method, score) in enumerate(sorted_methods, 1):
                print(f"   {rank}. {method}: {score:.3f}")
    
    return results


def test_topic_consistency_evaluation(results):
    """æµ‹è¯•ä¸»é¢˜ä¸€è‡´æ€§è¯„ä¼°"""
    print_separator("ä¸»é¢˜ä¸€è‡´æ€§è¯„ä¼°æµ‹è¯•")
    
    if not results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†æ®µç»“æœè¿›è¡Œè¯„ä¼°")
        return
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    model = SentenceTransformerModel(device='mps')
    text_processor = TextProcessor()
    evaluator = TopicConsistencyEvaluator(model, text_processor, "chinese")
    
    print_section("å„æ–¹æ³•ä¸€è‡´æ€§è¯„ä¼°")
    
    # å‡†å¤‡åˆ†æ®µç»“æœ
    segmentation_results = {}
    for method, result in results.items():
        if "paragraphs" in result:
            paragraphs = [p["text"] for p in result["paragraphs"]]
            segmentation_results[method] = paragraphs
            print(f"ğŸ“ {method}: {len(paragraphs)}æ®µ")
    
    # æ‰§è¡Œä¸€è‡´æ€§æ¯”è¾ƒ
    print_section("ä¸€è‡´æ€§æ¯”è¾ƒåˆ†æ")
    comparison_result = evaluator.compare_segmentation_consistency(segmentation_results)
    
    if "error" not in comparison_result.get("comparison_summary", {}):
        summary = comparison_result["comparison_summary"]
        detailed = comparison_result["detailed_results"]
        
        print(f"ğŸ† ä¸€è‡´æ€§æœ€ä½³æ–¹æ³•: {summary['best_method']}")
        print("\nğŸ“Š ä¸€è‡´æ€§è¯„åˆ†æ’å:")
        for rank, (method, score) in enumerate(summary['consistency_ranking'], 1):
            print(f"   {rank}. {method}: {score:.3f}")
        
        print("\nğŸ“‹ è¯¦ç»†ä¸€è‡´æ€§åˆ†æ:")
        for method, result in detailed.items():
            if "error" not in result:
                print(f"\n   ğŸ“ {method.upper()}:")
                print(f"      ç»¼åˆä¸€è‡´æ€§: {result['consistency_score']:.3f}")
                print(f"      è¯­ä¹‰ç›¸ä¼¼åº¦: {result['semantic_analysis']['avg_similarity']:.3f}")
                print(f"      ä¸»é¢˜è¿è´¯æ€§: {result['coherence_metrics']['topic_coherence']:.3f}")
                print(f"      æ®µè½å†…ä¸€è‡´æ€§: {result['paragraph_metrics']['avg_internal_consistency']:.3f}")
                
                if result['coherence_metrics']['detected_boundaries']:
                    print(f"      æ£€æµ‹åˆ°ä¸»é¢˜è¾¹ç•Œ: {result['coherence_metrics']['detected_boundaries']}")
    else:
        print(f"âŒ ä¸€è‡´æ€§è¯„ä¼°å¤±è´¥: {comparison_result['comparison_summary']['error']}")


def display_detailed_results(results):
    """æ˜¾ç¤ºè¯¦ç»†åˆ†æ®µç»“æœ"""
    print_separator("è¯¦ç»†åˆ†æ®µç»“æœå±•ç¤º")
    
    for method, result in results.items():
        if "paragraphs" in result:
            print_section(f"{method.upper()}æ–¹æ³•åˆ†æ®µç»“æœ")
            
            paragraphs = result["paragraphs"]
            print(f"ğŸ“Š æ€»æ®µè½æ•°: {len(paragraphs)}")
            print(f"ğŸ“ æ–‡æœ¬ç±»å‹: {result.get('text_type', 'unknown')}")
            print(f"â­ è´¨é‡åˆ†æ•°: {result['quality']['quality_score']:.3f}")
            print(f"ğŸ”§ ä½¿ç”¨æ–¹æ³•: {result.get('method', method)}")
            
            print("\næ®µè½å†…å®¹:")
            for i, paragraph in enumerate(paragraphs, 1):
                text = paragraph["text"]
                length = paragraph["length"]
                sentences = paragraph.get("sentence_count", "N/A")
                
                # æˆªæ–­æ˜¾ç¤º
                display_text = text if len(text) <= 100 else text[:100] + "..."
                print(f"   æ®µè½{i} (é•¿åº¦:{length}, å¥æ•°:{sentences}): {display_text}")


def test_chinese_optimization():
    """æµ‹è¯•ä¸­æ–‡ä¼˜åŒ–åŠŸèƒ½"""
    print_separator("ä¸­æ–‡è¯­è¨€ä¼˜åŒ–æµ‹è¯•")
    
    from src.utils.chinese_optimizer import ChineseTextOptimizer
    
    optimizer = ChineseTextOptimizer()
    
    # ä¸­æ–‡æµ‹è¯•æ–‡æœ¬
    chinese_text = "è§ç‚å¬äº†è¿™è¯ï¼Œå¿ƒä¸­ä¸ç¦ä¸€åŠ¨ã€‚\"åŸæ¥å¦‚æ­¤ã€‚\"ä»–è½»å£°è¯´é“ã€‚ç„¶åï¼Œè¯é¢˜çªç„¶è½¬å‘äº†ç°ä»£ç§‘æŠ€ã€‚äººå·¥æ™ºèƒ½çš„å‘å±•ç¡®å®ä»¤äººéœ‡æ’¼ã€‚è¿™äº›æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚"
    
    print_section("ä¸­æ–‡æ–‡æœ¬åˆ†æ")
    print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {chinese_text}")
    
    # ä¸­æ–‡æ¯”ä¾‹æ£€æµ‹
    chinese_ratio = optimizer.detect_chinese_ratio(chinese_text)
    print(f"ğŸ”¤ ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹: {chinese_ratio:.3f}")
    
    # å¢å¼ºåˆ†å¥
    sentences = optimizer.enhanced_chinese_sentence_split(chinese_text)
    print(f"ğŸ“œ å¢å¼ºåˆ†å¥ç»“æœ ({len(sentences)}å¥):")
    for i, sentence in enumerate(sentences, 1):
        print(f"   {i}. {sentence}")
    
    # å…³é”®è¯æå–
    keywords = optimizer.extract_chinese_keywords(chinese_text, 5)
    print(f"ğŸ·ï¸  ä¸­æ–‡å…³é”®è¯:")
    for word, pos, score in keywords:
        print(f"   {word} ({pos}): {score:.3f}")
    
    # å¯¹è¯æ£€æµ‹
    dialogues = optimizer.detect_dialogue_segments(chinese_text)
    print(f"ğŸ’¬ å¯¹è¯æ£€æµ‹:")
    for start, end, content in dialogues:
        print(f"   ä½ç½®{start}-{end}: \"{content}\"")
    
    # è¯­ä¹‰ç‰¹å¾
    features = optimizer.calculate_chinese_semantic_features(chinese_text)
    print(f"ğŸ“Š ä¸­æ–‡è¯­ä¹‰ç‰¹å¾:")
    for key, value in features.items():
        print(f"   {key}: {value:.3f}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºç‰ˆè¯­ä¹‰åˆ†æ®µç³»ç»Ÿç»¼åˆæµ‹è¯•")
    print(f"ğŸ”§ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ’» MPSå¯ç”¨: {torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False}")
    print(f"ğŸš€ CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    try:
        # 1. æµ‹è¯•æ‰€æœ‰åˆ†æ®µæ–¹æ³•
        results = test_all_segmentation_methods()
        
        # 2. æµ‹è¯•ä¸»é¢˜ä¸€è‡´æ€§è¯„ä¼°
        test_topic_consistency_evaluation(results)
        
        # 3. æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        display_detailed_results(results)
        
        # 4. æµ‹è¯•ä¸­æ–‡ä¼˜åŒ–
        test_chinese_optimization()
        
        print_separator("æµ‹è¯•å®Œæˆæ€»ç»“")
        print("ğŸ‰ æ‰€æœ‰åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        print("âœ… å¢å¼ºç‰ˆå¤šå°ºåº¦åˆ†æ®µå™¨")
        print("âœ… TransformerÂ²æ¶æ„åˆ†æ®µå™¨") 
        print("âœ… å¤šä»»åŠ¡å­¦ä¹ åˆ†æ®µå™¨")
        print("âœ… æ··åˆæ™ºèƒ½åˆ†æ®µå™¨")
        print("âœ… ä¸»é¢˜ä¸€è‡´æ€§è¯„ä¼°å™¨")
        print("âœ… ä¸­æ–‡è¯­è¨€ä¼˜åŒ–å™¨")
        print("\nğŸ¯ ç³»ç»Ÿå·²è¾¾åˆ°production-readyçŠ¶æ€ï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()