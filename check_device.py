#!/usr/bin/env python3
"""
æ£€æµ‹Macè®¾å¤‡åŠ é€Ÿèƒ½åŠ›
"""

import torch
import platform

def check_devices():
    """æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡"""
    print("=== Macè®¾å¤‡åŠ é€Ÿèƒ½åŠ›æ£€æµ‹ ===\n")
    
    print(f"ç³»ç»Ÿä¿¡æ¯: {platform.system()} {platform.machine()}")
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print()
    
    print("å¯ç”¨è®¾å¤‡:")
    
    # æ£€æµ‹CPU
    print("âœ… CPU: å§‹ç»ˆå¯ç”¨")
    
    # æ£€æµ‹CUDA (ä¸å¤ªå¯èƒ½åœ¨Macä¸Š)
    if torch.cuda.is_available():
        print("âœ… CUDA: å¯ç”¨")
        print(f"   è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   è®¾å¤‡{i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âŒ CUDA: ä¸å¯ç”¨")
    
    # æ£€æµ‹MPS (Metal Performance Shaders)
    if hasattr(torch.backends, 'mps'):
        if torch.backends.mps.is_available():
            print("âœ… MPS (Metal): å¯ç”¨ - æ¨èç”¨äºMacåŠ é€Ÿ")
            
            # æµ‹è¯•MPSè®¾å¤‡
            try:
                mps_device = torch.device("mps")
                test_tensor = torch.randn(100, 100).to(mps_device)
                print("   MPSæµ‹è¯•: é€šè¿‡")
            except Exception as e:
                print(f"   MPSæµ‹è¯•: å¤±è´¥ ({e})")
        else:
            print("âŒ MPS (Metal): ä¸å¯ç”¨")
    else:
        print("âŒ MPS (Metal): PyTorchç‰ˆæœ¬ä¸æ”¯æŒ")
    
    print()
    
    # è‡ªåŠ¨é€‰æ‹©æ¨èè®¾å¤‡
    def get_recommended_device():
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    recommended = get_recommended_device()
    print(f"æ¨èè®¾å¤‡: {recommended}")
    
    if recommended == "mps":
        print("\nğŸš€ ä½ çš„Macæ”¯æŒMPSåŠ é€Ÿï¼")
        print("å»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡: SEMANTIC_DEVICE=mps")
        print("æˆ–è€…åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®: SEMANTIC_DEVICE=mps")
        print("è¿™å°†æ˜¾è‘—æå‡BERTæ¨¡å‹çš„æ¨ç†é€Ÿåº¦")
    elif recommended == "cpu":
        print("\nâš ï¸  ä»…å¯ä½¿ç”¨CPUï¼Œæ¨ç†é€Ÿåº¦è¾ƒæ…¢")
        print("å¦‚æœä½ çš„Macæ”¯æŒMetalï¼Œè¯·å‡çº§PyTorchç‰ˆæœ¬")
    
    print()
    return recommended

def performance_test():
    """ç®€å•çš„æ€§èƒ½æµ‹è¯•"""
    print("=== è®¾å¤‡æ€§èƒ½æµ‹è¯• ===\n")
    
    devices_to_test = ["cpu"]
    
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        devices_to_test.append("mps")
    
    if torch.cuda.is_available():
        devices_to_test.append("cuda")
    
    import time
    
    for device_name in devices_to_test:
        print(f"æµ‹è¯•è®¾å¤‡: {device_name}")
        
        try:
            device = torch.device(device_name)
            
            # åˆ›å»ºæµ‹è¯•å¼ é‡
            start_time = time.time()
            x = torch.randn(1000, 768).to(device)
            y = torch.randn(768, 512).to(device)
            
            # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
            for _ in range(100):
                z = torch.mm(x, y)
            
            # ç¡®ä¿è®¡ç®—å®Œæˆ
            if device_name == "cuda":
                torch.cuda.synchronize()
            elif device_name == "mps":
                torch.mps.synchronize()
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            print(f"   100æ¬¡çŸ©é˜µä¹˜æ³•è€—æ—¶: {elapsed:.3f}ç§’")
            
        except Exception as e:
            print(f"   æµ‹è¯•å¤±è´¥: {e}")
        
        print()

def main():
    """ä¸»å‡½æ•°"""
    recommended_device = check_devices()
    
    print("=" * 50)
    performance_test()
    
    print("=" * 50)
    print("é…ç½®å»ºè®®:")
    print(f"export SEMANTIC_DEVICE={recommended_device}")
    print(f"æˆ–åœ¨.envæ–‡ä»¶ä¸­æ·»åŠ : SEMANTIC_DEVICE={recommended_device}")

if __name__ == "__main__":
    main()